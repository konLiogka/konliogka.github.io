<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, viewport-fit=cover, user-scalable=0"
    />
    <link rel="stylesheet" type="text/css" href="suckless.css" />
    <title>Data Mining with Python</title>
  </head>
  <body>
    <h1>Organisational Data Mining: Clustering and ML with Python</h1>
    <hr />
    <div class="banners">
      <p>[Python]</p>
      <p>[Data Mining]</p>
    </div>
    <p>
      Ths project was an assignment for the course "Organizational Data Mining"
      of IHU, IEE on the dataset "marketing.csv" for defining clusters of the
      samples and finding relationships between subjects based on their income.
      Written in python, using the libraries scikit learn for clustering and
      machine learning algorithms, matplotlib/seaborn for showing graphs and
      visualising data and apyori for apriori algorithm. You can find the code
      at my github page
      <a href="https://github.com/konLiogka/ODEPergasia" target="_blank"
        >here</a
      >
    </p>
    <h2>Required Libraries</h2>
    <div class="code-container">
        <p>
            <span class="string">from</span> <span class="variable">sklearn.cluster</span> <span class="string">import</span> <span class="variable">KMeans</span><br>
            <span class="string">from</span> <span class="variable">sklearn.cluster</span> <span class="string">import</span> <span class="variable">AgglomerativeClustering</span><br>
            <span class="string">from</span> <span class="variable">scipy.cluster.hierarchy</span> <span class="string">import</span> <span class="variable">dendrogram</span><br>
            <span class="string">from</span> <span class="variable">sklearn.cluster</span> <span class="string">import</span> <span class="variable">DBSCAN</span><br>
            <span class="string">from</span> <span class="variable">sklearn.neighbors</span> <span class="string">import</span> <span class="variable">NearestNeighbors</span><br>
            <span class="string">from</span> <span class="variable">pandas.plotting</span> <span class="string">import</span> <span class="variable">parallel_coordinates</span> <span class="string">as</span> <span class="variable">pc</span><br>
            <span class="string">from</span> <span class="variable">sklearn.model_selection</span> <span class="string">import</span> <span class="variable">train_test_split</span><br>
            <span class="string">from</span> <span class="variable">sklearn.metrics</span> <span class="string">import</span> <span class="variable">classification_report</span><br>
            <span class="string">from</span> <span class="variable">imblearn.over_sampling</span> <span class="string">import</span> <span class="variable">SMOTE</span><br>
            <span class="string">from</span> <span class="variable">sklearn.naive_bayes</span> <span class="string">import</span> <span class="variable">GaussianNB</span><br>
            <span class="string">from</span> <span class="variable">sklearn.neighbors</span> <span class="string">import</span> <span class="variable">KNeighborsClassifier</span><br>
            <span class="string">from</span> <span class="variable">sklearn</span> <span class="string">import</span> <span class="variable">tree</span><br>
            <span class="string">from</span> <span class="variable">sklearn.ensemble</span> <span class="string">import</span> <span class="variable">RandomForestClassifier</span><br>
            <span class="string">from</span> <span class="variable">apyori</span> <span class="string">import</span> <span class="variable">apriori</span><br>
            <span class="string">import</span> <span class="variable">sys</span><br>
            <span class="string">import</span> <span class="variable">matplotlib.pyplot</span> <span class="string">as</span> <span class="variable">plt</span><br>
            <span class="string">import</span> <span class="variable">numpy</span> <span class="string">as</span> <span class="variable">np</span><br>
            <span class="string">import</span> <span class="variable">pandas</span> <span class="string">as</span> <span class="variable">pd</span><br>
            <span class="string">import</span> <span class="variable">seaborn</span> <span class="string">as</span> <span class="variable">sns</span><br>
            <span class="string">import</span> <span class="variable">random</span><br>
        </p>
    </div>
   
    <hr />
    <h2>Task A - Imputation</h2>
    <p>
      The task involves addressing missing values in the provided dataset,
      "marketing.csv," marked as '?' due to incomplete responses from
      respondents. To handle this issue, the KNNImputer function from the
      sklearn library is utilized. This method effectively fills missing values
      by searching for neighboring samples with similar characteristics, so
      there aren't outliers. It performs well for the current dataset. It would
      need more computational power for large datasets as it computes every
      distance between every sample. Experimentation with different imputation
      values reveals that a value of 35 yields optimal results. Before
      proceeding with clustering, data analysis is conducted using seaborn
      library to visualize relationships between some features I've chosen.
    </p>
    <h2>Task B - Clustering</h2>
    <p>
      In order to obtain clear results in clustering algorithms, it's crucial to
      choose an appropriate number of clusters (K) to avoid overfitting and
      misinterpretation. For the KMEANS algorithm, the elbow method is employed
      to determine the optimal K value. This method calculates the sum of
      squared errors for different K values and visualizes them to identify the
      point where the line starts to resemble a linear trend or an elbow shape.
      For the dataset in question, K is determined to be between 13-15. After
      determining the appropriate K value, parallel coordinates plots are
      generated for 7 features to observe how many clusters should be formed
      based on where distortion starts to diminish. Applying the KMEANS
      algorithm, parallel coordinates plots for features are created, using the
      same number of plots as clusters for better visualization. Hierarchical
      clustering divides samples into clusters based on similarity.
      Visualization using dendrograms suggests 12 clusters if a horizontal line
      is drawn at the 100 level. For DBSCAN, the challenge lies in determining
      suitable values for min points and epsilon. A distance-based method like
      k-distances helps identify these values. In the provided example, min
      points is found to be 35 and epsilon is 3.
    </p>
    <h2>Task C - Classification</h2>
    <p>
      For proper classification of classes 8 and 9, minority oversampling using
      SMOTE is applied to increase the number of samples in these classes for
      the training sets. Training and testing sets are split 80%-20%.
    </p>
    <img src="images/odp1.png" />
    <p>
      KNN, a widely used machine learning algorithm, can perform well depending
      on the value of K. Here, I set K to 35. The accuracy reaches 39%, with
      improved recall and precision for classes 8 and 9. However, for large
      datasets, training might be slow due to the algorithm needing to compute
      distances between samples.
    </p>
    <img src="images/odp2.png" />
    <p>
      Decision tree classifiers categorize data based on various decisions made
      at each node, offering straightforward interpretation. While the accuracy
      remains unchanged, there is an improvement in recall and precision,
      indicating fewer false negatives and false positives.
    </p>
    <img src="images/odp3.png" />
    <p>
      Random forest performed the best (depth 15), which is essentially multiple
      decision trees. The output is decided by the output most trees agree with.
      SLightly better accuracy but overall better recall and precision.
    </p>
    <img src="images/odp4.png" />
    <h2>Task D - Apriori</h2>
    <p>
      The Apriori algorithm is utilized to discover correlations among the
      dataset by extracting rules. Lift, measuring the frequency of occurrence
      of associations relative to the random occurrence frequency, is employed
      here. All features are used, with manual mapping of numeric values to the
      responses. Setting the lift to 2.7 reveals various rules. Initially,
      individuals with low income are observed to live with their families.
      Conversely, married individuals tend to reside in the area for over 10
      years and own their homes, while those not in a relationship tend to rent
      apartments. Additionally, married couples have both single and dual
      income. Individuals with low income are either unemployed or students.
      English is the dominant language. Two-member families have their own
      homes. Individuals who own homes tend to be of Caucasian ethnicity. Those
      studying or in school live with their families.
    </p>
  </body>
</html>
